
---

## 📝 1. 핵심 요약

컴퓨터는 키보드로 입력받은 숫자를 실제 숫자가 아닌 **'문자(Character)'** 형태로 인식합니다. 이 영상은 프로그래밍 언어의 내장 함수(예: Python의 `int()`, C의 `atoi`)가 어떻게 문자열("123")을 실제 연산 가능한 정수(123)로 변환하는지 그 **저수준(Low-level) 원리와 알고리즘**을 설명합니다.

## 💡 2. 주요 개념 및 원리

### 2.1 데이터의 표현 (ASCII와 인코딩)

- 컴퓨터는 0과 1만 이해할 수 있으므로, 모든 문자(Character)는 숫자로 매핑되어 저장됩니다.
- 가장 기본적인 표준인 **ASCII(아스키) 코드**를 예로 들면:
    - 문자 `'0'`은 실제 숫자 0이 아니라, 메모리에 **48** (이진수 `00110000`)로 저장됩니다.
    - 문자 `'1'`은 **49**, `'2'`는 **50**... 순서대로 저장됩니다.

### 2.2 입력 버퍼의 함정

- 사용자가 키보드로 `5`와 `3`을 입력하면 컴퓨터는 이를 숫자 5, 3이 아닌 문자 `'5'`, `'3'`으로 인식합니다.
- **문제점**: 만약 이 상태에서 단순히 더하기 연산을 하면?
    - 문자 `'5'` (ASCII 53) + 문자 `'3'` (ASCII 51) = **104**
    - 우리가 원하는 결과인 **8**이 나오지 않습니다.

### 2.3 단일 자리 숫자 변환 알고리즘

- ASCII 테이블에서 숫자 문자들은 순서대로 배치되어 있습니다 (`'0'`=48, `'1'`=49 ...).
- **해결책**: 문자에서 `'0'`(48)을 빼면 실제 정수 값을 얻을 수 있습니다.
    - 예: `'5'` (53) - `'0'` (48) = **5** (정수)
- C언어 같은 저수준 언어에서는 이 원리를 직접 이용하며, Python 등에서는 `ord()` 함수 등을 내부적으로 활용합니다.

### 2.4 다릿수(Multi-digit) 문자열 변환 알고리즘 

- 문자열 `"4327"`을 숫자로 바꿀 때, 각 자릿수를 단순히 변환해서 나열하는 것이 아니라 **자릿수(10의 거듭제곱)** 개념을 적용해야 합니다.
- **컴퓨터의 처리 방식 (순차적 계산)**:
    1. 문자열을 앞에서부터 한 글자씩 읽습니다.
    2. 이전 값에 **10을 곱하고**(자릿수 올림), 현재 숫자를 더합니다.

## 🛠️ 3. 변환 알고리즘 로직 (Pseudo-code)

영상에서 설명하는 로직을 프로그래밍적으로 정리하면 다음과 같습니다.

```Python
def string_to_number(string):
    result = 0  # 결과를 저장할 변수
    
    for char in string:
        # 1. 문자를 숫자로 변환 (ASCII 값 보정)
        digit = char - '0'  # (실제 코드에서는 ord(char) - 48)
        
        # 2. 자릿수 올림 및 더하기
        # 기존 값에 10을 곱해 자릿수를 밀어주고, 현재 숫자를 더함
        result = result * 10 + digit
        
    return result
```

**실행 예시 ("437" 변환 과정):**

1. **'4' 읽음**: `0 * 10 + 4` = **4**
2. **'3' 읽음**: `4 * 10 + 3` = **43**
3. **'7' 읽음**: `43 * 10 + 7` = **437** → **최종 결과**

## 📌 4. 결론 및 인사이트

- 우리가 흔히 쓰는 `Integer.parseInt()`나 `int()` 같은 함수들은 마법 상자가 아니라, 내부적으로 이러한 **ASCII 값 연산과 자릿수 누적 알고리즘**을 통해 동작합니다.
- 이 원리를 알면 직접 문자열 파싱 함수를 구현하거나, 인코딩 관련 버그를 만났을 때 원인을 파악하는 데 도움이 됩니다.
- _참고: 반대의 과정(숫자 → 문자열 변환)은 이 과정의 역순으로 진행됩니다._