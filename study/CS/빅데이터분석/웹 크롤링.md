
> 자동화된 프로그램(웹 크롤러, 스파이더, 봇)**을 이용해 인터넷상의 웹 페이지들을 체계적으로 탐색하고, 정보를 수집하여 색인화하는 과정

### 주요 작동 방식

1. **시작**: 웹 크롤러는 미리 정해진 URL 목록(시드)에서 탐색을 시작합니다.
2. **탐색**: 웹 페이지를 방문하면, 페이지 안의 하이퍼링크를 발견하고 이를 따라 계속해서 새로운 페이지를 찾아다닙니다.
3. **수집**: 발견한 페이지의 HTML 코드를 분석하여 필요한 정보를 추출하고, 데이터베이스에 저장합니다.
4. **반복**: 이 과정을 끊임없이 반복하며 웹상의 방대한 데이터를 수집합니다.

### 실습

개요

- 목표: 웹 페이지 HTML을 가져와 파싱하고, 특정 요소를 추출해 콘솔 출력 및 CSV로 저장
- 사용 라이브러리: requests, bs4(BeautifulSoup), csv
- 실습 대상: Papago 메인 페이지 HTML 가져오기, YES24 일간 베스트셀러 책 제목 크롤링

환경 준비

- Python 버전: Colab 기본 Python
- 설치/임포트

- requests: HTTP 요청
- bs4: HTML 파싱
- csv: 데이터 파일 저장

requests 기본 사용법

핵심 포인트

- requests.get(url, headers=…, timeout=…)로 HTTP GET 요청
- status_code 확인하여 성공 여부 판단
- User-Agent 헤더로 실제 브라우저처럼 보이게 설정 가능

예시 코드, 유의사항

- timeout으로 무한 대기 방지
- 서비스 정책(TOS) 준수, 공격적 크롤링 금지
- 과도한 빈도 요청은 차단 위험

BeautifulSoup(bs4)로 HTML 파싱

핵심 포인트

- BeautifulSoup(html, ‘html.parser’ 또는 ‘lxml’)
- CSS 선택자 select(), 단일 요소 select_one()
- 텍스트는 get_text(strip=True)

- 예시: Papago 메인 HTML 가져오기

  

- 예시: YES24 베스트셀러 제목 추출

  

- bs4 텍스트 처리 팁

- get_text(separator=” “, strip=True): 자식 텍스트를 공백으로 합치고 앞뒤 공백 제거
- .text는 단순 문자열, .get_text()는 옵션 제공

1. CSV로 저장하기

- 핵심 포인트

- csv.writer로 행 단위 기록
- newline=’’과 encoding=‘utf-8’ 필수로 깨짐/줄바꿈 문제 예방

- 예시: 임시 데이터 CSV 저장

  

- 예시: YES24 베스트셀러를 CSV로 저장

  

1. 에러/엣지 케이스 대응

- 상태 코드가 200이 아닌 경우: resp.raise_for_status() 또는 조건문으로 처리
- 구조 변경 감지: 선택자(select)가 빈 결과면 사이트 구조가 바뀐 것일 수 있음

- 해결: 개발자 도구로 최신 CSS 선택자 확인 후 업데이트

- 한글/인코딩 문제: encoding=‘utf-8’로 파일 저장, 페이지 응답의 .encoding 확인
- 차단/봇 감지: User-Agent 조정, 요청 간 딜레이 추가, 과도한 병렬 요청 피하기

1. 베스트 프랙티스 체크리스트

- User-Agent 헤더 설정
- timeout 지정
- 예외 처리(resp.raise_for_status)
- 파서 선택: ‘lxml’ 권장(설치 필요 시 !pip install lxml)
- 데이터 정제: get_text(strip=True) 사용
- 파일 입출력: newline=’’, encoding=‘utf-8’
- 사이트 이용약관 및 로봇 정책 준수

1. 전체 워크플로우 통합 예제

- 목적: YES24 베스트셀러 수집 → 콘솔 출력 → CSV 저장

  

1. 다음 확장 아이디어

- 페이지네이션 자동 반복 수집(pageNumber 증가)
- 상세 페이지로 들어가 저자/출판사/가격까지 크롤링
- 데이터프레임(pandas)로 변환 후 정렬/필터링
- 스케줄러로 주기적 수집 및 Google Sheets 업로드