
> 자동화된 프로그램(웹 크롤러, 스파이더, 봇)**을 이용해 인터넷상의 웹 페이지들을 체계적으로 탐색하고, 정보를 수집하여 색인화하는 과정

### 주요 작동 방식

1. **시작**: 웹 크롤러는 미리 정해진 URL 목록(시드)에서 탐색을 시작합니다.
2. **탐색**: 웹 페이지를 방문하면, 페이지 안의 하이퍼링크를 발견하고 이를 따라 계속해서 새로운 페이지를 찾아다닙니다.
3. **수집**: 발견한 페이지의 HTML 코드를 분석하여 필요한 정보를 추출하고, 데이터베이스에 저장합니다.
4. **반복**: 이 과정을 끊임없이 반복하며 웹상의 방대한 데이터를 수집합니다.


# 🐍 Colab 실습: Python 웹 크롤링 (requests + bs4 + csv) 정리

## 🚀 개요

- **🎯 목표:** 웹 페이지 HTML을 가져와 파싱하고, 특정 요소를 추출해 콘솔 출력 및 CSV로 저장
    
- **📚 사용 라이브러리:** `requests`, `bs4(BeautifulSoup)`, `csv`
    
- **🔍 실습 대상:** Papago 메인 페이지 HTML, YES24 일간 베스트셀러 책 제목 크롤링
    

---

## 🛠️ 환경 준비

- **Python 버전:** Colab 기본 Python
    
- **설치/임포트:**
    
    - `requests`: HTTP 요청
    - `bs4`: HTML 파싱
    - `csv`: 데이터 파일 저장
        

---

## 1. `requests` 기본 사용법

> 💡 **핵심 포인트**
> 
> - `requests.get(url, headers=…, timeout=…)`로 HTTP GET 요청
> - `status_code` 확인하여 성공 여부 판단 (e.g., `200`)
> - `User-Agent` 헤더로 실제 브라우저처럼 보이게 설정

### 🐍 예시 코드


```Python
import requests

url = "https://www.example.com"
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
}

resp = requests.get(url, headers=headers, timeout=10)

print(resp.status_code)   # 200이면 성공
print(resp.text[:500])    # 응답 HTML 일부 미리보기
```

> ⚠️ **유의사항**
> 
> - `timeout`으로 무한 대기 방지
> - 서비스 정책(TOS) 준수, 공격적 크롤링 금지
> - 과도한 빈도 요청은 차단 위험

---

## 2. `BeautifulSoup` (bs4)로 HTML 파싱

> 💡 **핵심 포인트**
> 
> - `BeautifulSoup(html, ‘html.parser’ 또는 ‘lxml’)`
>     
> - CSS 선택자 `select()`, 단일 요소 `select_one()`
>     
> - 텍스트는 `get_text(strip=True)`
>     

### 🐍 예시 1: Papago 메인 HTML 가져오기

Python

```
import requests
from bs4 import BeautifulSoup

url = "https://papago.naver.com"
resp = requests.get(url)
soup = BeautifulSoup(resp.text, 'html.parser')

print(soup.title.get_text())    # 페이지 제목
print(soup.find('div', id='root')) # 특정 id의 요소
```

### 🐍 예시 2: YES24 베스트셀러 제목 추출

Python

```
import requests
from bs4 import BeautifulSoup

url = "https://www.yes24.com/product/category/daybestseller?categoryNumber=001&pageNumber=1&pageSize=24&type=day"
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
}

resp = requests.get(url, headers=headers)
if resp.status_code == requests.codes.ok:
    soup = BeautifulSoup(resp.text, 'lxml')  # 빠르고 엄격한 파서
    names = soup.select("#yesBestList a.gd_name")  # CSS 선택자로 책 제목 링크 선택
    for i, name in enumerate(names, 1):
        print(f"{i}위: {name.get_text(strip=True)}")
else:
    print("요청 실패:", resp.status_code)
```

### 📌 bs4 텍스트 처리 팁

- `get_text(separator=” “, strip=True)`: 자식 텍스트를 공백으로 합치고 앞뒤 공백 제거
    
- `.text`는 단순 문자열, `.get_text()`는 옵션 제공
    

---

## 3. CSV로 저장하기

> 💡 **핵심 포인트**
> 
> - `csv.writer`로 행 단위 기록
>     
> - `newline=''`과 `encoding='utf-8'` 필수로 깨짐/줄바꿈 문제 예방
>     

### 🐍 예시 1: 임시 데이터 CSV 저장

Python

```
import csv

csv_path = "output.csv"

with open(csv_path, 'w', newline='', encoding='utf-8') as csvFile:
    writer = csv.writer(csvFile)
    writer.writerow(['First', 'Second', 'Third'])
    writer.writerow(['jwa', 30, 100])

print("CSV 저장 완료:", csv_path)
```

### 🐍 예시 2: YES24 베스트셀러를 CSV로 저장

Python

```
import requests
from bs4 import BeautifulSoup
import csv

url = "https://www.yes24.com/product/category/daybestseller?categoryNumber=001&pageNumber=1&pageSize=24&type=day"
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
}

resp = requests.get(url, headers=headers, timeout=10)
resp.raise_for_status()  # 실패 시 예외 발생

soup = BeautifulSoup(resp.text, 'lxml')
names = soup.select("#yesBestList a.gd_name")

rows = []
for i, name in enumerate(names, 1):
    title = name.get_text(strip=True)
    rows.append([i, title])

csv_path = "yes24_bestsellers.csv"
with open(csv_path, 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(["rank", "title"])
    writer.writerows(rows)

print(f"저장 완료: {csv_path} (총 {len(rows)}권)")
```

---

## 4. 에러 / 엣지 케이스 대응

- **상태 코드가 200이 아닌 경우:** `resp.raise_for_status()` 또는 조건문으로 처리
    
- **구조 변경 감지:** 선택자(`select`)가 빈 결과면 사이트 구조가 바뀐 것일 수 있음
    
    - **해결:** 개발자 도구로 최신 CSS 선택자 확인 후 업데이트
        
- **한글/인코딩 문제:** `encoding='utf-8'`로 파일 저장, 페이지 응답의 `.encoding` 확인
    
- **차단/봇 감지:** `User-Agent` 조정, 요청 간 딜레이 추가, 과도한 병렬 요청 피하기
    

---

## ✅ 베스트 프랙티스 체크리스트

- [ ] `User-Agent` 헤더 설정
    
- [ ] `timeout` 지정
    
- [ ] 예외 처리 (`resp.raise_for_status()`)
    
- [ ] 파서 선택: `lxml` 권장 (설치 필요 시 `!pip install lxml`)
    
- [ ] 데이터 정제: `get_text(strip=True)` 사용
    
- [ ] 파일 입출력: `newline=’’`, `encoding=‘utf-8’`
    
- [ ] 사이트 이용약관 및 로봇 정책(robots.txt) 준수
    

---

## 🧩 전체 워크플로우 통합 예제

**목적:** YES24 베스트셀러 수집 → 콘솔 출력 → CSV 저장

Python

```
import requests
from bs4 import BeautifulSoup
import csv
import time

URL = "https://www.yes24.com/product/category/daybestseller?categoryNumber=001&pageNumber=1&pageSize=24&type=day"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
}
CSV_PATH = "yes24_bestsellers.csv"

def fetch_html(url: str) -> str:
    """지정된 URL의 HTML을 가져옵니다."""
    resp = requests.get(url, headers=HEADERS, timeout=10)
    resp.raise_for_status() # 요청 실패 시 예외 발생
    return resp.text

def parse_titles(html: str) -> list[list]:
    """HTML을 파싱하여 [순위, 제목] 리스트를 반환합니다."""
    soup = BeautifulSoup(html, 'lxml')
    names = soup.select("#yesBestList a.gd_name")
    rows = []
    for i, name in enumerate(names, 1):
        title = name.get_text(strip=True)
        rows.append([i, title])
    return rows

def save_csv(rows: list[list], path: str) -> None:
    """데이터를 CSV 파일로 저장합니다."""
    with open(path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["rank", "title"])
        writer.writerows(rows)

def main():
    try:
        html = fetch_html(URL)
        rows = parse_titles(html)
        
        if not rows:
            print("데이터를 찾을 수 없습니다. 사이트 구조가 변경되었을 수 있습니다.")
            return

        print("--- YES24 베스트셀러 ---")
        for r in rows:
            print(f"{r[0]}위: {r[1]}")
        
        save_csv(rows, CSV_PATH)
        print(f"\n✅ 완료: {CSV_PATH}, 총 {len(rows)}권")

    except requests.exceptions.RequestException as e:
        print(f"HTTP 요청 실패: {e}")
    except Exception as e:
        print(f"오류 발생: {e}")


if __name__ == "__main__":
    main()
```

---

## 🚀 다음 확장 아이디어

- 페이지네이션 자동 반복 수집 (`pageNumber` 증가)
    
- 상세 페이지로 들어가 저자/출판사/가격까지 크롤링
    
- 데이터프레임(pandas)로 변환 후 정렬/필터링
    
- 스케줄러로 주기적 수집 및 Google Sheets 업로드